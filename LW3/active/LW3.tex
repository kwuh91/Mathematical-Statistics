\documentclass[a4paper, 14pt]{extarticle}

\usepackage{../../latexDependencies/misc/preamble2}

\geometry{a4paper}

% Название дисциплины
\newcommand{\subject}{Теория вероятности и математическая статистика} 

% Тип работы
% lab - для лабораторной работы 
% hw  - для домашней     работы
\newcommand{\task}{lab} 

% Номер работы
\newcommand{\taskNumber}{3} 

% Название работы
\newcommand{\taskNameOne}{Моделирование выборки из абсолютно непрерывного } 
\newcommand{\taskNameTwo}{закона распределения методом обратных функций.} 

% Имя студента
\newcommand{\studentName}{Очкин Н.В.}

% Имя преподававателя
\newcommand{\teacherName}{Облакова Т.В.}

% Группа
\newcommand{\group}{ФН11-52Б}

% Вариант
\newcommand{\variant}{9}

\begin{document}

\graphicspath{ {../../latexDependencies/images} } 
\input{../../latexDependencies/frontmatter/titlepage2}

\newgeometry{left=25mm, right=25mm, top=20mm, bottom=20mm}

\graphicspath{ {../../latexDependencies/images/LW3} }

% Customize section, subsection, subsubsection and paragraph styles
\titleformat{\section}
  {\normalfont\large\bfseries}{\thesection}{1em}{}

\titleformat{\subsection}
  {\normalfont\normalsize\bfseries}{\thesubsection}{1em}{}

\titleformat{\subsubsection}
  {\normalfont\small\bfseries}{\thesubsubsection}{1em}{}

\titleformat{\paragraph}
  {\small\small\bfseries}{\theparagraph}{1em}{}

\thispagestyle{empty}

\null\newpage

\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}

\pagenumbering{roman}

\tableofcontents
\newpage

\pagenumbering{arabic}
\setcounter{page}{1}

\setstretch{1}
\linespread{1.1}

\setlength{\parindent}{0pt}

\fontsize{12pt}{16pt}\selectfont

\definecolor{myblue}{HTML}{0A88C2}
\definecolor{myred}{HTML}{FF1B1C}
\definecolor{mygreen}{HTML}{386641}

\lstdefinestyle{mystyle}{
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{myblue},
    stringstyle=\color{myred},
    commentstyle=\color{green!50!black},
    showstringspaces=false,
    frame=leftline, 
    framesep=10pt, 
}

% Set the style for Python code
\lstset{style=mystyle, extendedchars=\true}

% --------------------------------------START--------------------------------------

\section{Задание}\vspace{-20pt}\rule{\linewidth}{0.1mm}

\begin{enumerate}
  \item Для данного $n$ методом обратных функций смоделируйте выборку 
  из закона распределения с заданной плотностью  $p(x)$.
  \item Для полученной выборки найдите гистограмму относительных частот. 
  Постройте на одном рисунке графики теоретической плотности $p(x)$ и 
  гистограмму относительных частот.
  \item Вычислите выборочное среднее и выборочную дисперсию и сравните с 
  истинными значениями этих характеристик.
  \item Используя неравенство \high{Dvoretzky-Kiefer-Wolfowitz}, 
  постройте 90\% доверительный интервал для функции распределения $F(x)$.
\end{enumerate}

Приведите графическую иллюстрацию

\section{Исходные данные}\vspace{-20pt}\rule{\linewidth}{0.1mm}

\begin{equation*}
  \text{Вариант: }9 \qquad n: 120
\end{equation*}
\begin{equation}
  \scalebox{1.25}{$p(x) = \cfrac{1}{\sqrt{0.4 \pi}x} e^{-(\ln{x} - 2)^2 / 0.4}, \quad x > 0$}
\end{equation}

\section{Решение}

\subsection{Часть 1}\vspace{-20pt}\rule{\linewidth}{0.1mm}
Для данного $n$ методом обратных функций смоделируйте выборку 
из закона распределения с заданной плотностью  $p(x)$.\\

\subsubsection{Функция распределения}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Найдем функцию распределения:
\begin{equation}
    F_X(x) = \int_{-\infty}^{x} f_X(t) dt, \quad \text{где}
\end{equation}
$f_X(x)$ - плотность распределения.\\

Подставим (1) в (2):
\begin{gather*}
    F_X(x) = 
    \int_{0}^{x} \cfrac{1}{\sqrt{0.4 \pi}y} e^{-(\ln{y} - 2)^2 / 0.4} dy = \\[1em]
    = \left[\hspace{5pt}
    \begin{aligned}
        & t          = \frac{\ln(y)-2}{\sqrt{0.4}}          \qquad & dt &   = \frac{1}{y \sqrt{0.4}} dy   \\
        & \ln(y) - 2 = t \sqrt{0.4}                         \qquad & dy &   = y \sqrt{0.4} dt             \\
        & \ln(y)     = t \sqrt{0.4} + 2                     \qquad & x: & \hspace{5pt} t = \frac{\ln(x)-2}{\sqrt{0.4}} \\
        & y          = \exp \left[ t \sqrt{0.4} + 2 \right] \qquad & 0: & \hspace{5pt} t = -\infty                     \\ 
    \end{aligned}\,
    \hspace{5pt}\right] = \\[1em]
    = \cfrac{1}{\sqrt{0.4 \pi}} \int_{-\infty}^{\frac{\ln(x)-2}{\sqrt{0.4}}} e^{\left[ -t \sqrt{0.4} -2 \right]} 
    \cdot e^{-t^2} \cdot e^{\left[ t \sqrt{0.4} +2 \right]} \cdot \sqrt{0.4} dt = \\[1em]
    = \cfrac{1}{\sqrt{\pi}} \int_{-\infty}^{\frac{\ln(x)-2}{\sqrt{0.4}}} e^{-t^2} dt 
    = \frac{1}{\sqrt{\pi}} \left( \int_{-\infty}^{0} e^{-t^2} dt + \int_{0}^{\frac{\ln(x)-2}{\sqrt{0.4}}} e^{-t^2} dt \right) = \\[1em]
    = \frac{1}{\sqrt{\pi}} \left( \frac{\pi}{2} \text{erf} (t) \bigg|^0_{-\infty} + \frac{\sqrt{\pi}}{2} \cdot 
    \text{erf} \left( \cfrac{\ln(x)-2}{\sqrt{0.4}} \right) \right) \oeq
\end{gather*}\\
\vspace{10pt}
\hdashrule[0.5ex][c]{1\textwidth}{0.4pt}{3mm}
\begin{center}
    где erf(x) - \textbf{функция ошибок} (также называемая функция ошибок Гаусса).\\
\end{center}

\begin{minipage}[c]{0.5\textwidth}
    \begin{equation*}
        \scalebox{1.25}{$\text{erf} (x) = \cfrac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt$}
    \end{equation*}
\end{minipage}
\hfill
\begin{minipage}[c]{0.5\textwidth}
    \includegraphics[width=1\textwidth]{Error_Function.svg}
\end{minipage}\\

{\footnotesize \textbf{Примечание:} из графика видно, что erf(0) = 0, erf($-\infty$) = -1}

\hdashrule[0.5ex][c]{1\textwidth}{0.4pt}{3mm}

\begin{gather*}
    \oeq \hspace{3pt} \frac{1}{\pi} \left( \frac{\pi}{2} \left( 0 - \left( -1 \right) \right) + 
    \frac{\pi}{2} \cdot \text{erf} \left( \cfrac{\ln(x)-2}{\sqrt{0.4}} \right) \right) = \\[1em]
    = \frac{1}{\pi} \left( \frac{\pi}{2} + \frac{\pi}{2} \cdot \text{erf} \left( 
    \cfrac{\ln(x) - 2}{\sqrt{0.4}} \right) \right) = \\[1em]
    = \cfrac{1}{2} + \cfrac{1}{2} \text{erf} \left( \cfrac{\ln(x) - 2}{\sqrt{0.4}} \right)
\end{gather*}\\
\vspace{-10pt}
В конечном итоге, функция распределения имеет вид\\[1em]
\begin{equation}
    F_X(x) = \cfrac{1}{2} + \cfrac{1}{2} \text{erf} \left( \cfrac{\ln(x) - 2}{\sqrt{0.4}} \right)
\end{equation}

\subsubsection{Обратная функция}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Так как для нахождения обратной функции распределения требуется найти обратную 
функцию ошибок, что аналитически сделать сложно, воспользуемся численными методами.

\paragraph{Метод Ньютона}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Для нахождения обратной функции воспользуемся методом касательных (Ньютона). \\
Рабочая формула
\begin{equation*}
  x_{n+1} = x_n - \cfrac{f(x_n)}{f'(x_n)}
\end{equation*}
Вообще говоря, метод используется для нахождения корня заданной функции. Так что 
для нахождения обратной функции $y = f(x)$, т.е. $x = f^{-1}(y)$ будем искать 
решение уравнения: $f(x) - y = 0$
\begin{equation}
  x_{n+1} = x_n - \cfrac{f(x_n) - y}{(f(x_n) - y)'_x} = 
  x_n - \cfrac{f(x_n) - y}{f'(x_n)}
\end{equation}
Погрешность $\varepsilon$ возьмем равной 1e-6.

\paragraph{Метод центральных разностей}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Производные будем искать методом центральных разностей.\\
Рабочая формула
\begin{equation}
  f'(x) \approx \cfrac{f(x+h) - f(x-h)}{2h}
\end{equation}
Погрешность определяется как $O(h)$, $h$ примем равной 1e-6.\\

Подставив (5) в (4), получим:
\begin{equation}
  x_{n+1} = x_n - \cfrac{(f(x_n) - y) \cdot 2 h}{f(x_n + h) - f(x_n - h)}
\end{equation}

\subsubsection{Реализация численного нахождения обратной функции}

\paragraph{Реализация метода центральных разностей}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Реализуем на языке программирования python метод центральных разностей (5):

\begin{center}
  \begin{lstlisting}[language=Python, 
                   caption={Реализация метода центральных разностей}, 
                   label={lst:CDM}]
class CDM:
  def __init__(self, h):
      self.h = h
  
  def diff(self, f, x):
      numerator = f(x + self.h) - f(x - self.h)
      denominator = 2 * self.h

      return numerator / denominator
  \end{lstlisting}
\end{center}

\paragraph{Реализация метода Ньютона}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Теперь реализуем метод Ньютона (4), используя метод центральных разностей 
(листинг \ref{lst:CDM}):

\begin{center}
  \begin{lstlisting}[language=Python, 
  caption={Реализация метода Ньютона}, 
  label={lst:Newton}]
class Newton:
  def __init__(self, f, CDM_object, tol=1e-6, max_iter=1000):
      self.f = f
      self.CDM = CDM_object
      self.tol = tol 
      self.max_iter = max_iter

  def solve(self, y, x0):
      x = x0
      for _ in range(self.max_iter):
          f_x = self.f(x) - y
          f_prime_x = self.CDM.diff(self.f, x)
          if abs(f_prime_x) < 1e-10:
              raise ValueError("Derivative is zero, method fails.")
          x_new = x - f_x / f_prime_x
          if abs(x_new - x) < self.tol:
              return x_new
          x = x_new

      raise ValueError(f"Method did not converge.({x_new})")
  \end{lstlisting}
\end{center}

\paragraph{Реализация нахождения обратной функции}\vspace{-20pt}\rule{\linewidth}{0.1mm}

В конечном итоге получим:

\begin{center}
  \begin{lstlisting}[language=Python, 
                   caption={Реализация нахождения обратной функции}, 
                   label={lst:inverse}]
if __name__ == '__main__':
  def cdf(x): # F_X
    return float(1/2 + 1/2 * \
        scipy.special.erf((np.log(x) - 2)/(np.sqrt(0.4))))

  cdm    = CDM(h=1e-6)
  newton = Newton(cdf, cdm, tol=1e-6, max_iter=1000)

  def inverse(y, x0): # x = f^-1(y)
      return newton.solve(y, x0)
  \end{lstlisting}
\end{center}
\vspace{10pt}
где\\ 
функция cdf - программная запись, найденной ранее функции распределения (3);\\
функция inverse - функция, возвращающее значение обратной функции к (3) в точке.\\

{\footnotesize \textbf{Примечание:} Библиотеки scipy и numpy используются только 
для доступа к функции ошибок, натуральному логарифму и квадратному корню.}

\subsubsection{Генерация псевдослучайных чисел}

\paragraph{Линейный конгруэнтный метод}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Для генерации случайных величин воспользуемся одним из методов генерации псевдослучайных чисел - 
\textbf{Линейным конгруэнтным методом}.\\
Суть метода заключается в вычислении последовательности случайных чисел $X_n$, полагая
\begin{equation}
  X_{n+1} = (aX_n + c)\hspace{3pt} \text{mod} \hspace{3pt} m, \quad \text{где}
\end{equation}
$m$ - модуль ($m \geq 2$); \\
$a$ - множитель ($0 \leq a < m$); \\
$c$ - приращение ($0 \leq c < m$); \\
$X_0$ - начальное значение ($0 \leq X_0 < m$).\\

За значениями параметров обратимся к [\ref{item:source1}].
\begin{equation}
  m = 2^{(60)} - 93 \qquad a = 561860773102413563 \qquad c = 0.
\end{equation}
В случае когда $c = 0$, метод называют \textbf{мультипликативным конгруэнтным методом}.

\paragraph{Реализация ЛКМ}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Реализуем линейный конгруэнтный метод (7), используя параметры (8):

\begin{center}
  \begin{lstlisting}[language=Python, caption={Реализация ЛКМ}, label={lst:LCG}]
class LCG:
  def __init__(self, 
               seed, a=561860773102413563, c=0, m=2**60-93):
      self.seed = seed
      self.a = a
      self.c = c
      self.m = m
      self.state = seed

  def next(self):
      self.state = (self.a * self.state + self.c) % self.m
      return self.state / self.m # Normalize to [0, 1)
  \end{lstlisting}
\end{center}

\paragraph{Моделирование выборки}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Наконец смоделируем 120 случайных величин в виде вектора линейным конгруэнтным методом:\\
\begin{center}
  \begin{lstlisting}[language=Python]
n = 120
lcg = LCG(seed=340751464)

data = [lcg.next() for _ in range(n)]
print(data)
  \end{lstlisting}
\end{center}
\vspace{10pt}
Начальное значение (seed) в ЛКМ выбирается так, чтобы $x_0 \neq 0$. Это необходимо для того, чтобы 
последовательность была полной длины, т.е. имела максимальную периодичность 
при генерации чисел. Обычно используют случайное или произвольно выбранное 
значение из множества $\{1, ..., m - 1\}$ [\ref{item:source1}]. 

\noindent
\begin{adjustbox}{max width=1\textwidth}
  \parbox{\linewidth}{%
    \begin{gather*}
      Y = [ \\
      \begin{aligned}
        & 0.32949885091783276, & \hspace{3pt} & 0.9732846125910063,  & \hspace{3pt} & 0.39434856188646605, & \hspace{3pt} & 0.8210789016402354,  & \\
        & 0.20093003622010405, & \hspace{3pt} & 0.9707650441880256,  & \hspace{3pt} & 0.4178790819080603,  & \hspace{3pt} & 0.2974690498690837,  & \\
        & 0.32632062605066997, & \hspace{3pt} & 0.8137561621450644,  & \hspace{3pt} & 0.6418089688930682,  & \hspace{3pt} & 0.72226998934102,    & \\
        & 0.12543257092465954, & \hspace{3pt} & 0.39665152743167287, & \hspace{3pt} & 0.7205668938187388,  & \hspace{3pt} & 0.18456086494051507, & \\ 
      \end{aligned} \\
      ...\\
      ]
    \end{gather*}
  }
\end{adjustbox}

Теперь пересчитаем полученный вектор случайных величин, в соответствии с функцией inverse из 
листинга \ref{lst:inverse}. \\
Однако сперва подеберем вектор начальных приближений, так как того требует метод Ньютона.

\vspace{-80pt}
\includegraphics[width=1\textwidth]{cdf}
\vspace{-80pt}

Из графика видно, что функция (3) приблизительно принимает значения $0 < x < 20$ при $0 < y < 1$. 
Исходя из этого подберем вектор начальных приближений: \\ \null
[0, 3, 6, 9, 12, 15, 18, 21]. \\

Итого имеем:

\begin{center}
  \begin{lstlisting}[language=Python]
guesses = [0, 3, 6, 9, 12, 15, 18, 21]
for ind, el in enumerate(data):
    for attempt, guess in enumerate(guesses):
        try: 
            inv_value = inverse(el, guess)
            data[ind] = inv_value
            break
        except:
            pass

        if attempt == len(guesses) - 1:
            raise Exception('Solution was not found')
  \end{lstlisting}
\end{center}

\noindent
\begin{adjustbox}{max width=1\textwidth}
  \parbox{\linewidth}{%
    \begin{gather*}
      X = [ \\
      \begin{aligned}
        & 6.065674809818662, & \hspace{3pt} & 17.52728100897831,  & \hspace{3pt} & 6.5544583429545265, & \hspace{3pt} & 11.147396579310449, & \\ 
        & 5.078922433676263, & \hspace{3pt} & 17.222193164730466, & \hspace{3pt} & 6.734763210632847,  & \hspace{3pt} & 5.825351333431677,  & \\
        & 6.041854304433931, & \hspace{3pt} & 11.010347184551701, & \hspace{3pt} & 8.692598700648851,  & \hspace{3pt} & 9.618384853081634,  & \\
        & 4.421534190647852, & \hspace{3pt} & 6.572007701239677,  & \hspace{3pt} & 9.596593105982482,  & \hspace{3pt} & 4.944860000874664,  & \\
      \end{aligned} \\
      ...\\
      ]
    \end{gather*}
  }
\end{adjustbox}

\subsection{Часть 2}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Для полученной выборки найдите гистограмму относительных частот. 
Постройте на одном рисунке графики теоретической плотности $p(x)$ и 
гистограмму относительных частот.

\subsubsection{Первоначальная обработка полученных статистических данных}

\paragraph{Крайние члены вариационного ряда и размах выборки}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Найдем крайние члены вариационного ряда как минимальное и максимальное значения 
набора данных, а также размах выборки, как их разницу:

\vspace{10pt}

\begin{center}
  \begin{lstlisting}[language=Python]
mini, maxi = min(data), max(data)
print(mini, maxi)

range_ = maxi - mini
print(range_)
  \end{lstlisting}
\end{center}

\vspace{-5pt}

\begin{align*}
  & \text{Крайние члены: }  2.1028, \hspace{5pt} 23.4245 \\
  & \text{Размах выборки: }  21.3217
\end{align*}

{\footnotesize \textbf{Примечание:} Выводимые данные округлены до 4х знаков для удобства чтения.}

\newpage

\paragraph{Группировка данных}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Для начала определим количество интервалов, воспользовавшись правилом
Стерджеса:
\begin{equation*}
    k = 1 + \lfloor \log_2 n \rfloor,
\end{equation*}
где $n$ — общее число наблюдений величины, \\ 
$\log_2$ — логарифм по основанию 2, \\
$\lfloor x \rfloor$ — обозначает целую часть числа $x$. \\

И определим шаг интервала разделив размах выборки на количество интервалов:

\vspace{10pt}

\begin{center}
  \begin{lstlisting}[language=Python]
trunc = lambda x : int(str(x)[:str(x).index('.')])
k = 1 + trunc(np.log2(n))
h = range_ / k
  \end{lstlisting}
\end{center}

\vspace{-5pt}

\begin{align*}
    & \text{Количество интервалов: } 7 \\
    & \text{Шаг интервала: }  3.046
\end{align*}

Теперь сгруппируем данные:

\vspace{10pt}

\begin{center}
  \begin{lstlisting}[language=Python]
grouped_data = []

begin = mini
for i in range(k):
    end = begin + h

    middle = (begin + end) / 2
    freq = sum(begin <= el < end for el in data)
    
    if i == k - 1:
        freq += 1

    relative_freq = freq / n

    grouped_element = {
        'interval numero': i,
        'interval': f'[{begin}, {end})',
        'middle': middle,
        'frequency': freq,
        'relative frequency': relative_freq
    }
    grouped_data.append(grouped_element)

    begin = end
  \end{lstlisting}
\end{center}
\vspace{10pt}
Полученную группировку представим в виде таблицы:
\vspace{10pt}
\begin{table}[h!]
  \centering
  \renewcommand{\arraystretch}{1.5}
  \begin{adjustbox}{max width=0.8\textwidth}
      \begin{tabular}{|c|c|c|c|c|c|}
      \hline
      номер     & \multirow{2}{*}{интервал} & середина  & \multirow{2}{*}{частота} & относительная \\
      интервала &                           & интервала &                          & частота       \\
      \hline
      0 & [2.1028, 5.1488)   & 3.6258  & 30 & 0.25    \\
      \hline
      1 & [5.1488, 8.1947)   & 6.6718  & 46 & 0.3833  \\
      \hline
      2 & [8.1947, 11.2407)  & 9.7177  & 24 & 0.2     \\
      \hline
      3 & [11.2407, 14.2867) & 12.7637 & 11 & 0.09167 \\
      \hline
      4 & [14.2867, 17.3326) & 15.8096 & 6  & 0.05    \\
      \hline
      5 & [17.3326, 20.3786) & 18.8556 & 2  & 0.0167  \\
      \hline
      6 & [20.3786, 23.4245) & 21.9016 & 1  & 0.00833 \\
      \hline
      \end{tabular}
  \end{adjustbox}
  \caption{Сгруппированные данные}
  \label{tab:your_table_label}
\end{table}

\paragraph{Гистограмма относительных частот}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Построим на одном рисунке графики теоретической плотности (1) и гистограмму относительных
частот. \\
По оси абсцисс для гистограммы укажем середины интервалов, \\
по оси ординат - вектор относительных частот, разделенный на шаг интервала: \\

\begin{center}
  \begin{lstlisting}[language=Python]
x_axis = [el['middle']                 for el in grouped_data]
y_axis = [el['relative frequency'] / h for el in grouped_data]
  \end{lstlisting}
\end{center}

\begin{align*}
  & \text{int}: [ 3.6258 \hspace{10pt} 6.6718  \hspace{10pt} 9.7177 \hspace{10pt} 12.7637 \hspace{10pt} 15.8096 \hspace{10pt} 18.8556 \hspace{10pt} 21.9016] \\
  & \frac{\text{p}_k}{\text{h}}: [0.0821  \hspace{10pt} 0.1258  \hspace{10pt} 0.0657 \hspace{10pt} 0.0301  \hspace{10pt} 0.0164  \hspace{10pt} 0.0055  \hspace{10pt} 0.0027]
\end{align*}

Для построения графиков воспользуемся библиотекой \high{matplotlib}. \\

\begin{center}
  \begin{lstlisting}[language=Python]
import matplotlib.pyplot as plt

def pdf(x):
    return 1 / (np.sqrt(0.4 * np.pi) * x) \
        * np.exp(-(np.log(x) - 2)**2 / 0.4)

def buildBar(x, y):

  # histogtamm 
  plt.bar(x, y, color='white', edgecolor='black')

  # pdf
  x_values = np.linspace(0.01, trunc(maxi), 1000)
  y_values = pdf(x_values)
  plt.plot(x_values, y_values, color='red', linestyle='-', 
                                            linewidth=1.5)

  plt.show()

buildBar(x_axis, y_axis)
  \end{lstlisting}
\end{center}

{\footnotesize \textbf{Примечание:} код был несколько упрощен, чтобы не загромождать текст, 
полный код см. в приложении.}

\vfill
\includegraphics[width=1\textwidth]{histXpdf}
\vfill

\newpage

\subsection{Часть 3}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Вычислите выборочное среднее и выборочную дисперсию и сравните с 
истинными значениями этих характеристик.

\subsubsection{Эмпирические и теоретические характеристики}

\paragraph{Математическое ожидание}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Запишем формулу для математического ожидания:
\begin{equation}
  \mathbb{E}[X] = \int_{-\infty}^{\infty} x f_X (x) dx, \qquad \text{где}
\end{equation}
$f_X (x)$ - плотность распределения.\\ 

\paragraph{Метод интегрирования Монте-Карло}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Для вычисления интеграла воспользуемся численным методом интегрирования Монте-Карло
\begin{equation}
  \int_{a}^{b} f(x) dx \approx \cfrac{b-a}{N} \sum_{i=1}^{N} f(u_i), \quad \text{где}
\end{equation}
$u$ - равномерно распредленная на отрезке интегрирования $[a, b]$ случайная величина.\\

Геометрическая интерпретация данного метода похожа на известный детерминистический метод, 
с той разницей, что вместо равномерного разделения области интегрирования на маленькие интервалы 
и суммирования площадей получившихся «столбиков» мы забрасываем область интегрирования случайными 
точками, на каждой из которых строим такой же «столбик», определяя его ширину как 
$\cfrac{b - a}{N}$, и суммируем их площади.\\

Точность оценки данного метода зависит только от количества точек $N$.\\

\paragraph{Реализация метода Монте-Карло}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Так как данный метод опирается на генерацию случайных чисел на промежутке, расширим 
функционал нашей реализации ЛКМ (листинг \ref{lst:LCG}) и добавим следующий метод:

\vspace{10pt}

\begin{center}
  \begin{lstlisting}[language=Python]
def next_in_range(self, a, b):
  return a + (b - a) * self.next()
  \end{lstlisting}
\end{center}

\vspace{10pt}

Теперь реализуем интегрирование методом Монте-Карло, используя  
описаннный ЛКМ:\\

\begin{center}
  \begin{lstlisting}[language=Python, caption={Реализация метода Монте-Карло}, label={lst:MonteCarlo}]
class MonteCarlo:
  def __init__(self, N, PRNG_object):
      self.N = int(N)
      self.PRNG = PRNG_object
  
  def integrate(self, f, a, b):
      mult = (b - a) / self.N
      
      generatedValues = []
      for _ in range(self.N):
          randomArg = self.PRNG.next_in_range(a, b)
          randomFuncVal = f(randomArg)

          generatedValues.append(randomFuncVal)
      
      return mult * sum(generatedValues)
  \end{lstlisting}
\end{center}

\paragraph{Реализация численного нахождения математического ожидания}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Прежде чем реализовывать вычисление самого интеграла, заметим, что в пределах 
интегрирования (9) присутствует бесконечность, что затрудняет интегрирование 
методом Монте-Карло (10). \\
Воспользуемся заменой, чтобы свести бесконечные пределы в конечные:
\begin{gather*}
  \mathbb{E}[X] = \int_{-\infty}^{+\infty} x f_X(t)dt = \\[1em]
  = \left[\hspace{5pt}
    \begin{gathered}
    x       = \tan(t)                             \\
    t       = \arctan(x)                          \\
    dx      = \cfrac{1}{\cos^2(t)} dt             \\
    -\infty : t=\arctan(-\infty)= -\cfrac{\pi}{2} \\ 
    +\infty : t=\arctan(+\infty)= \cfrac{\pi}{2}  \\
    \end{gathered}\,
  \hspace{5pt}\right] = \\[1em]
  = \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \tan(t) \cdot f_X(\tan(t)) \cdot \cfrac{1}{\cos^2(t)} dt
\end{gather*}

Итого получим:
\begin{equation}
  \mathbb{E}[X] = \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} g(t) dt, 
  \qquad g(t) = \tan(t) \cdot f_X(\tan(t)) \cdot \cfrac{1}{\cos^2(t)}
\end{equation}

Объединим теперь (11) и (10) и получим:
\begin{equation}
  \mathbb{E}[X] = \int_{-\infty}^{+\infty} x f_X(x)dx = 
  \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} g(t) dt \approx 
  \cfrac{\pi}{N} \sum_{i=1}^N g(u_i), \quad \text{где}
\end{equation}
$g(x) = \tan(x) \cdot f_X(\tan(x)) \cdot \cfrac{1}{\cos^2(x)}$, \\[1em]
$u_i$ ищем в соответствии с (листинг \ref{lst:LCG}).\\

Подставляя (1) в (12) и (8) в (7):
\begin{gather*}
  \mathbb{E}[X] = \int_{0}^{+\infty} x f_X(x)dx = 
  \int_{0}^{\frac{\pi}{2}} \tan(t) f_X(\tan(t)) \cfrac{1}{\cos^2(t)} dt \approx \\[1em]
  \approx \cfrac{\pi / 2}{N} \sum_{i=1}^{N} \left[ \tan(u_i) \cdot 
  \cfrac{1}{\sqrt{0.4 \pi} \tan(u_i)} e^{-(\ln(\tan(u_i)) - 2)^2 /0.4}  
  \cdot \cfrac{1}{\cos^2(u_i)} \right], \quad \text{где}
\end{gather*}
$u_{i} = (561860773102413563 \cdot u_{i-1})\hspace{3pt} \text{mod} 
\hspace{3pt} 2^{60} - 93$ \\\\

При программной реализации, как уже было сказано ранее, 
N отвечает за точность полученной оценки метода, так 
что чем оно больше, тем лучше.\\

\begin{center}
  \begin{lstlisting}[language=Python]
monteCarlo = MonteCarlo(1e7, lcg)

def subs(t):
    return np.tan(t) * pdf(np.tan(t)) * (1 / np.cos(t)**2) 

ExpectedValue = monteCarlo.integrate(subs, 0, np.pi/2)
  \end{lstlisting}
\end{center}
\vspace{10pt}
где классы \textbf{LCG} и \textbf{MonteCarlo} представлены в листингах 
\ref{lst:LCG} и \ref{lst:MonteCarlo} соответственно.\\

Итого получаем:
\begin{equation*}
  \mathbb{E}[X] \approx 8.16
\end{equation*}

\paragraph{Дисперсия}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Аналогично найдем дисперсию, как 
\begin{equation*}
  \text{D}[X] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2
\end{equation*}

\begin{center}
  \begin{lstlisting}[language=Python]
def subs2(t):
return np.tan(t)**2 * pdf(np.tan(t)) * (1 / np.cos(t)**2) 

Var = monteCarlo.integrate(subs2, 0, np.pi/2) - \
  monteCarlo.integrate(subs, 0, np.pi/2)**2
  \end{lstlisting}
\end{center}
\vspace{10pt}
Итого получаем:
\begin{equation*}
  \text{D}[X] \approx 14.65
\end{equation*}

\paragraph{Выборочное среднее}\vspace{-20pt}\rule{\linewidth}{0.1mm}

\begin{equation*}
  \overline{X} = \cfrac{1}{n} \sum_{k=1}^{n} X_k 
\end{equation*}

\begin{center}
  \begin{lstlisting}[language=Python]
OverlineX = sum(data)/n
  \end{lstlisting}
\end{center}
\vspace{10pt}
Итого получаем:
\begin{equation*}
  \overline{X} \approx 7.88
\end{equation*}

\paragraph{Выборочная дисперсия}\vspace{-20pt}\rule{\linewidth}{0.1mm}

\begin{equation*}
  S^2 = \cfrac{1}{n-1} \sum_{k=1}^{n} \left( X_k - \overline{X} \right)^2
\end{equation*}

\begin{center}
  \begin{lstlisting}[language=Python]
S2 = 1 / (n - 1) * sum([(x - OverlineX)**2 for x in data])
  \end{lstlisting}
\end{center}
\vspace{10pt}
Итого получаем:
\begin{equation*}
  S^2 \approx 15.36
\end{equation*}

\paragraph{Сравнение}\vspace{-20pt}\rule{\linewidth}{0.1mm}

\begin{equation*}
  \left| \mathbb{E}[X] - \overline{X} \right| \hspace{100pt}
  \sqrt{\cfrac{\text{D}[X]}{S^2}}
\end{equation*}

\begin{center}
  \begin{lstlisting}[language=Python]
diff1 = abs(ExpectedValue - OverlineX)
diff2 = np.sqrt(Var/S2)
  \end{lstlisting}
\end{center}
\vspace{10pt}

Итого имеем:
\begin{alignat*}{6}
  & \mathbb{E}[X] &&= 8.16 &\hspace{50pt} \overline{X} &&= 15.36 &\hspace{50pt} \left| \mathbb{E}[X] - \overline{X} \right| &&= 0.2792 \\
  & \text{D}[X] &&= 14.65 &\hspace{50pt} S^2 &&= 15.36 &\hspace{50pt} \sqrt{\frac{\text{D}[X]}{S^2}} &&= 0.9768
\end{alignat*}

Поскольку абсолютная величина разности математического ожидания 
и выборочного среднего мала, а отношение выборочной 
дисперсии к ее теоретическому значению близко к единице, то 
результаты моделирования можно признать удовлетворительными.

\subsection{Часть 4}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Используя неравенство \high{Dvoretzky-Kiefer-Wolfowitz}, 
постройте 90\% доверительный интервал для функции распределения $F(x)$.

\paragraph{Неравенство Dvoretzky-Kiefer-Wolfowitz}\vspace{-20pt}\rule{\linewidth}{0.1mm}

\begin{equation*}
  P \left( \sup_{x \in \mathbb{R}} \left| \hat{F}_n(x) - F(x) \right| > \varepsilon \right) 
  \leq 2 e^{-2 n \varepsilon^2}
\end{equation*}

Таким образом, если $2 e^{-2 n \varepsilon^2} = \alpha$, $\ln(\frac{2}{\alpha}) = 
2 n \varepsilon^2$, $\varepsilon = \sqrt{\frac{1}{2 n} \ln(\frac{2}{\alpha})}$, то 
с вероятностью $1 - \alpha$
\begin{equation*}
  L(x) \leq \hat{F}_n(x) \leq R(x),
\end{equation*}
где
\begin{equation*}
  L(x) = \max \left\{ \hat{F}_n (x) - \sqrt{\frac{1}{2 n} \ln\left(\frac{2}{\alpha}\right)}, 0 \right\} 
  \hspace{40pt} 
  R(x) = \min \left\{ \hat{F}_n (x) + \sqrt{\frac{1}{2 n} \ln\left(\frac{2}{\alpha}\right)}, 1 \right\}
\end{equation*}

\paragraph{Реализация функций}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Так как требуется построить 90\% доверительный интервал, $\alpha$ возьмем равной 0.1.\\

\begin{center}
  \begin{lstlisting}[language=Python]
def Fempir(x):
  ind = lambda x : 1 if x > 0 else 0

  return sum([ind(x - X)/n for X in data])

alpha = 0.1
epsilon = np.sqrt(1/(2*n) * np.log(2/alpha))

def L(x):
  return max(Fempir(x) - epsilon, 0)

def R(x):
  return min(Fempir(x) + epsilon, 1)
  \end{lstlisting}
\end{center}
\vspace{10pt}

\paragraph{Графическая иллюстрация}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Построим доверительный интервал уровня 0.9 для функции распределения на 
основе неравенства Дворецкого - Кифера - Волфовица.\\

\begin{center}
  \begin{lstlisting}[language=Python]
def buildPlots():
  x_values = np.linspace(0.01, trunc(maxi) + 1, 1000)

  # empir
  empir_y_values = [Fempir(x) for x in x_values]
  plt.plot(x_values, empir_y_values)

  # theoretical
  cdf_y_values = [cdf(x) for x in x_values]
  plt.plot(x_values, cdf_y_values)

  # L
  L_y_values = [L(x) for x in x_values]
  plt.plot(x_values, L_y_values)

  # R
  R_y_values = [R(x) for x in x_values]
  plt.plot(x_values, R_y_values)

  # Show the plot
  plt.show()
  \end{lstlisting}
\end{center}
\vspace{10pt}

{\footnotesize \textbf{Примечание:} код был несколько упрощен, чтобы не загромождать текст, 
полный код см. в приложении.}

\vspace{30pt}

\includegraphics[width=1\textwidth]{Dvoretzky-Kiefer-Wolfowitz}

\section{Вывод}\vspace{-20pt}\rule{\linewidth}{0.1mm}

В ходе проделанной лабораторной работы было проведено моделирование выборки из 
логнормального распределения методом обратных функций, реализованы такие численные 
методы, как метод Ньютона, метод центральных разностей и метод Монте-Карло. Был реализован 
алгоритм генерации псевдослучайных чисел. На основе значений выборочного среднего и 
выборочной дисперсии был сделан вывод о степени качества моделирования. Также был 
построен доверительный интервал на основе неравенства Дворецкого - Кифера - Вольфовица.

% ---------------------------------------CODE---------------------------------------

\newpage

\section{Приложение}\vspace{-20pt}\rule{\linewidth}{0.1mm}

Программный код, с помощью которого была выполнена данная лабораторная работа.\\

\begin{center}
  \begin{lstlisting}[language=Python]
import warnings
warnings.filterwarnings("ignore", category=RuntimeWarning)


class CDM:
    def __init__(self, h):
        self.h = h
    
    def diff(self, f, x):
        numerator = f(x + self.h) - f(x - self.h)
        denominator = 2 * self.h

        return numerator / denominator


class Newton:
    def __init__(self, f, CDM_object, tol=1e-6, max_iter=1000):
        self.f = f
        self.CDM = CDM_object
        self.tol = tol 
        self.max_iter = max_iter

    def solve(self, y, x0):
        x = x0
        for _ in range(self.max_iter):
            f_x = self.f(x) - y
            f_prime_x = self.CDM.diff(self.f, x)
            if abs(f_prime_x) < 1e-10:
                raise ValueError("Derivative is zero, method fails.")
            x_new = x - f_x / f_prime_x
            if abs(x_new - x) < self.tol:
                return x_new
            x = x_new

        raise ValueError(f"Method did not converge.({x_new})")


class LCG:
    def __init__(self, seed, a=561860773102413563, c=0, m=2**60-93):
        self.seed = seed
        self.a = a
        self.c = c
        self.m = m
        self.state = seed

    def next(self):
        self.state = (self.a * self.state + self.c) % self.m
        return self.state / self.m  # Normalize to [0, 1)
    
    def next_in_range(self, a, b):
        return a + (b - a) * self.next()


class MonteCarlo:
    def __init__(self, N, PRNG_object):
        self.N = int(N)
        self.PRNG = PRNG_object
    
    def integrate(self, f, a, b):
        mult = (b - a) / self.N
        
        generatedValues = []
        for _ in range(self.N):
            randomArg = self.PRNG.next_in_range(a, b)
            randomFuncVal = f(randomArg)

            generatedValues.append(randomFuncVal)
        
        return mult * sum(generatedValues)

  
import scipy.special
import numpy as np


if __name__ == '__main__':

    # --------------------------PART1--------------------------

    def cdf(x): # F_X
        return float(1/2 + 1/2 * \
            scipy.special.erf((np.log(x) - 2)/(np.sqrt(0.4))))

    cdm    = CDM(h=1e-6)
    newton = Newton(cdf, cdm, tol=1e-6, max_iter=1000)

    def inverse(y, x0): # x = f^-1(y)
        return newton.solve(y, x0)

    n = 120

    lcg = LCG(seed=340751464)

    # --------------------------PART2--------------------------

    data = [lcg.next() for _ in range(n)]
    # print(f'Y: {data}')

    guesses = [0, 3, 6, 9, 12, 15, 18, 21]
    for ind, el in enumerate(data):
        for attempt, guess in enumerate(guesses):
            try: 
                inv_value = inverse(el, guess)
                data[ind] = inv_value
                break
            except:
                pass

            if attempt == len(guesses) - 1:
                raise Exception('Solution was not found')
    
    # print(f'X: {data}')

    # --------------------------PART3--------------------------

    mini, maxi = min(data), max(data)
    # print(f'min: {mini}, max: {maxi}')

    range_ = maxi - mini
    # print(f'range: {range_}')

    # --------------------------PART4--------------------------

    trunc = lambda x : int(str(x)[:str(x).index('.')])

    k = 1 + trunc(np.log2(n))
    # print(f'k: {k}')
    
    h = range_ / k
    # print(f'h: {h}')

    # --------------------------PART5--------------------------

    grouped_data = []

    begin = mini
    for i in range(k):
        end = begin + h

        middle = (begin + end) / 2
        freq = sum(begin <= el < end for el in data)
        
        if i == k - 1:
            freq += 1

        relative_freq = freq / n

        grouped_element = {
            'interval numero': i,
            'interval': f'[{np.round(begin, 4)}, {np.round(end, 4)})',
            'middle': np.round(middle, 4),
            'frequency': freq,
            'relative frequency': relative_freq
        }
        grouped_data.append(grouped_element)

        begin = end

    # for element in grouped_data:
    #     print(element['interval numero'], 
    #           element['interval'], 
    #           element['middle'], 
    #           element['frequency'], 
    #           element['relative frequency'])

    # --------------------------PART6--------------------------

    import matplotlib.pyplot as plt

    def pdf(x):
        return 1 / (np.sqrt(0.4 * np.pi) * x) \
            * np.exp(-(np.log(x) - 2)**2 / 0.4)

    def buildBar(x, y):
        # Define colors
        RED   = '#6F1D1B'

        # Define font sizes
        SIZE_TICKS   = 10

        # Create the figure and axis
        _, ax = plt.subplots(figsize=(10, 6))

        # histogtamm
        ax.bar(x, y, width=3.05, color='none', 
                                 edgecolor='black', 
                                 linewidth=1.5)

        # pdf
        x_values = np.linspace(0.01, trunc(maxi) + 1, 1000)
        y_values = pdf(x_values)
        ax.plot(x_values, y_values, color=RED, 
                                    linestyle='-', 
                                    linewidth=1.5)

        # axis names
        ax.set_xlabel('int')
        ax.set_ylabel('$\\frac{p_k}{h}$', fontsize=20)

        # ticks settings
        xticks = [i for i in range(0, trunc(maxi) + 2, 3)]
        ax.set_xticks(xticks)

        # Adjust the font size of the tick labels
        ax.tick_params(axis='both', which='major', 
                                    labelsize=SIZE_TICKS)

        # Update font settings
        plt.rcParams.update({'font.family': 'serif', 
                             'font.size': 12})

        # Adjust layout
        plt.tight_layout()

        # Save the figure
        plt.savefig('histXpdf.png', dpi=300, transparent=True)

        # Show the plot
        plt.show()

    x_axis = [el['middle']                 for el in grouped_data]
    y_axis = [el['relative frequency'] / h for el in grouped_data]

    # print(f'x: {np.round(x_axis, 4)}')
    # print(f'y: {np.round(y_axis, 4)}')

    # buildBar(x_axis, y_axis)

    # --------------------------PART7--------------------------

    monteCarlo = MonteCarlo(1e7, lcg)

    def subs(t):
        return np.tan(t) * pdf(np.tan(t)) * (1 / np.cos(t)**2) 

    ExpectedValue = monteCarlo.integrate(subs, 0, np.pi/2)
    
    # print(ExpectedValue)

    # --------------------------PART8--------------------------

    def subs2(t):
        return np.tan(t)**2 * pdf(np.tan(t)) * (1 / np.cos(t)**2) 

    Var = monteCarlo.integrate(subs2, 0, np.pi/2) - \
          monteCarlo.integrate(subs, 0, np.pi/2)**2
    
    # print(Var)

    # --------------------------PART9--------------------------

    OverlineX = sum(data)/n

    # print(f'OverlineX: {OverlineX}')

    S2 = 1 / (n - 1) * sum([(x - OverlineX)**2 for x in data])

    # print(f'S2: {S2}')

    # --------------------------PART10--------------------------

    diff1 = abs(ExpectedValue - OverlineX)
    diff2 = np.sqrt(Var/S2)

    # print(diff1)
    # print(diff2)

    # --------------------------PART11--------------------------

    def Fempir(x):
        ind = lambda x : 1 if x > 0 else 0

        return sum([ind(x - X)/n for X in data])

    alpha = 0.1
    epsilon = np.sqrt(1/(2*n) * np.log(2/alpha))

    def L(x):
        return max(Fempir(x) - epsilon, 0)
    
    def R(x):
        return min(Fempir(x) + epsilon, 1)
    
    # --------------------------PART12--------------------------

    def buildPlots():
        # Define colors
        RED    = '#6F1D1B'
        BLUE   = '#12EAEA'
        GREEN  = '#2E5339'
        PURPLE = '#8D80AD'

        # Define font sizes
        SIZE_TICKS   = 10

        # Create the figure and axis
        _, ax = plt.subplots(figsize=(10, 6))  # Adjust the figure size as needed

        x_values = np.linspace(0.01, trunc(maxi) + 1, 1000)

        # empir
        empir_y_values = [Fempir(x) for x in x_values]
        ax.plot(x_values, empir_y_values, color=RED, 
                                          linestyle='-', 
                                          linewidth=1.5, 
                                          label='Fempir(x)')

        # theoretical
        cdf_y_values = [cdf(x) for x in x_values]
        ax.plot(x_values, cdf_y_values, color=BLUE, 
                                          linestyle='-', 
                                          linewidth=1.5, 
                                          label='Theoretical(x)')

        # L
        L_y_values = [L(x) for x in x_values]
        ax.plot(x_values, L_y_values, color=GREEN, 
                                          linestyle='-', 
                                          linewidth=1.5, 
                                          label='L(x)')

        # R
        R_y_values = [R(x) for x in x_values]
        ax.plot(x_values, R_y_values, color=PURPLE, 
                                          linestyle='-', 
                                          linewidth=1.5, 
                                          label='R(x)')

        # axis names
        ax.set_xlabel('x')

        # ticks settings
        xticks = [i for i in range(0, trunc(maxi) + 2, 3)]
        ax.set_xticks(xticks)

        # Adjust the font size of the tick labels
        ax.tick_params(axis='both', which='major', 
                                    labelsize=SIZE_TICKS)

        plt.legend(fontsize=10, loc='best')

        # Update font settings
        plt.rcParams.update({'font.family': 'serif', 
                             'font.size': 12})

        # Adjust layout
        plt.tight_layout()

        # Save the figure
        plt.savefig('Dvoretzky-Kiefer-Wolfowitz.png', 
                    dpi=300, transparent=True)

        # Show the plot
        plt.show()

    buildPlots()
  \end{lstlisting}
\end{center}

% ------------------------------------LITERATURE------------------------------------

\vfill

\section{Список использованных источников}\vspace{-20pt}\rule{\linewidth}{0.1mm}
\begin{enumerate}
  \item \label{item:source1} L'Ecuyer, Pierre (January 1999). "Tables of Linear Congruential Generators of Different Sizes and Good Lattice Structure" - С. 256
\end{enumerate}

\end{document}

% Как известно, математическое ожидание логнормального распределения определяется следующей формулой:
% \begin{equation}
%   \mathbb{E}[X] = e^{[\mu + \sigma^2/2]}
% \end{equation}

% Для того чтобы определить параметры $\mu$ и $\sigma$, сравним найденную функцию распределения с 
% общей формулой функции распределения для логнормального распределения.

% \begin{equation*}
%   F_X(x) = \cfrac{1}{2} + \cfrac{1}{2} \text{erf} \left( \cfrac{\ln(x) - \mu}{\sigma \sqrt{2}} \right)
%   \qquad
%   F_X(x) = \cfrac{1}{2} + \cfrac{1}{2} \text{erf} \left( \cfrac{\ln(x) - 2}{\sqrt{0.4}} \right)
% \end{equation*}

% Откуда можно заключить, что 
% \begin{equation}
%   \mu = 2 \qquad \sigma = \sqrt{2}
% \end{equation}

% Подставив (10) в (9), получим:

% \begin{equation}
%   \mathbb{E}[X] = e^3 \approx 20.0855
% \end{equation}
